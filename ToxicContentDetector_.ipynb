{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test Copy of ToxicContentDetector .ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szMXQg7lfrgZ",
        "colab_type": "text"
      },
      "source": [
        "# Detecting Slang Using BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_oT-kssfzYp",
        "colab_type": "text"
      },
      "source": [
        "### Installing Hugging Face library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCcR-xrEowGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CL0yF_V_odLN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from tqdm.notebook import tqdm\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "import transformers\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1DY7VNEJ962",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "#print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kwN5740o8IS",
        "colab_type": "text"
      },
      "source": [
        "# Downloading data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYXDgobj7JvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://abcom.com/public-data/tutorials/ml-toxic/jigsawdata.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0YLT-fW7U6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip '/content/jigsawdata.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gifFeMlqpO3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv(\"/content/bert toxix/train.csv\")\n",
        "test = pd.read_csv('/content/bert toxix/test.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11J1VyWXCvp5",
        "colab_type": "text"
      },
      "source": [
        "# Examining data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_8ze8MtSDlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train[train['toxic']==1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3D_La4d5nPz",
        "colab_type": "text"
      },
      "source": [
        "### Target distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkdI2cTr4c5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns=['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
        "zeros =[]\n",
        "ones =[]\n",
        "for col in columns:\n",
        "  zeros.append(train[col].value_counts()[0])\n",
        "  ones.append(train[col].value_counts()[1])\n",
        "  \n",
        "  \n",
        "df = pd.DataFrame({'zero': zeros,'one': ones}, index=columns)\n",
        "df.plot.bar(rot=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyqdfRXzuCtL",
        "colab_type": "text"
      },
      "source": [
        "# Build the model\n",
        "Instantiating TFAutoModel, AutoConfig and AutoTokenizer will directly create a class of the relevant BERT architecture "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoecxsFMyYv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(transformer, loss, max_len=512):\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    sequence_output = transformer(input_word_ids)[0]\n",
        "    cls_token = sequence_output[:, 0, :]\n",
        "    x = tf.keras.layers.Dropout(0.35)(cls_token)\n",
        "    out = Dense(1, activation='sigmoid')(x)\n",
        "    \n",
        "    model = Model(inputs=input_word_ids, outputs=out)\n",
        "    model.compile(Adam(lr=3e-5), loss=loss, metrics=[tf.keras.metrics.AUC()])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXqUoxnR62Nl",
        "colab_type": "text"
      },
      "source": [
        "# The focal loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25W6o-cBpIu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def focal_loss(gamma=2., alpha=.2):\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "        return -K.mean(alpha * \n",
        "                       K.pow(1. - pt_1, gamma) * \n",
        "                       K.log(pt_1)) - K.mean((1 - alpha) * \n",
        "                       K.pow(pt_0, gamma) * \n",
        "                       K.log(1. - pt_0))\n",
        "    return focal_loss_fixed\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7Lhfo1w65P-",
        "colab_type": "text"
      },
      "source": [
        "# Instantiating model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eByghsNt2oMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "    transformer_layer = transformers.TFBertModel.from_pretrained('bert-base-uncased')\n",
        "    model = build_model(transformer_layer, loss=focal_loss(gamma=1.5), max_len=512)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DG-kW66E6J9",
        "colab_type": "text"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5IC2aabFFGm",
        "colab_type": "text"
      },
      "source": [
        "### Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgHk__Csygmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First load the real tokenizer\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Save the loaded tokenizer locally\n",
        "save_path = 'distilbert_base_uncased/'\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "# Reload it with the huggingface tokenizers library\n",
        "fast_tokenizer = BertWordPieceTokenizer('distilbert_base_uncased/vocab.txt', lowercase=True)\n",
        "fast_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggg7LDGHbM8T",
        "colab_type": "text"
      },
      "source": [
        "### Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAuEIRZ1ySUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n",
        "    tokenizer.enable_truncation(max_length=maxlen)\n",
        "    tokenizer.enable_padding(length=maxlen)\n",
        "    all_ids = []\n",
        "    \n",
        "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
        "        text_chunk = texts[i:i+chunk_size].tolist()\n",
        "        encs = tokenizer.encode_batch(text_chunk)\n",
        "        all_ids.extend([enc.ids for enc in encs])\n",
        "    \n",
        "    return np.array(all_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC-ptcvky8PS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = fast_encode(train.comment_text.astype(str), fast_tokenizer, maxlen=512)\n",
        "x_test = fast_encode(test.comment_text.astype(str), fast_tokenizer, maxlen=512)\n",
        "y = train.toxic.values\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xr_gaBE5si9",
        "colab_type": "text"
      },
      "source": [
        "# Preparing datasets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM1PCCgPqEuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE=64\n",
        "\n",
        "train_dataset = (\n",
        "    tf.data.Dataset \n",
        "      .from_tensor_slices((x, y))\n",
        "      .repeat()\n",
        "      .shuffle(2048)\n",
        "      .batch(BATCH_SIZE)\n",
        "    # AUTOTUNE prompts the runtime to prepare the next set \n",
        "    # while processing the current one\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE) \n",
        ")\n",
        "\n",
        "test_data = (\n",
        "    tf.data.Dataset# create dataset\n",
        "    .from_tensor_slices(x_test) \n",
        "    .batch(BATCH_SIZE)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BARpyVXx69f8",
        "colab_type": "text"
      },
      "source": [
        "# training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwxBZGTW27Ow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "  train_history = model.fit(\n",
        "      train_dataset,\n",
        "      \n",
        "      steps_per_epoch=150,\n",
        "      \n",
        "      epochs=10\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqqQPCkaBuNs",
        "colab_type": "text"
      },
      "source": [
        "# Predicting on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjIWaCpSuZ5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test['toxic'] = model.predict(test_data, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DI7A-FWGowB",
        "colab_type": "text"
      },
      "source": [
        "Save it to CSV and load it "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlySBK2CBzMX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test.to_csv('test.csv', index=False)\n",
        "data=pd.read_csv('/content/test.csv')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cQNY6T41OJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def replace(toxic):\n",
        "  if toxic >=0.5:\n",
        "    toxic=1\n",
        "  else:\n",
        "    toxic=0\n",
        "  return toxic\n",
        "\n",
        "test['prediction']=test['toxic'].apply(lambda x : replace(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVvGPaHr2aGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PbJPJC93IvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test.prediction.value_counts().plot(kind='bar')\n",
        "plt.xlabel('toxic or non-toxic')\n",
        "plt.ylabel('count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEvwK9X0kOhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text1=test.comment_text[186]\n",
        "text1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXZHF_-CDkDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n",
        "    tokenizer.enable_truncation(max_length=maxlen)\n",
        "    tokenizer.enable_padding(length=maxlen)\n",
        "    all_ids = []\n",
        "    \n",
        "    #for i in tqdm(range(0, len(texts), chunk_size)):\n",
        "    #text_chunk = texts[i:i+chunk_size].tolist()\n",
        "    encs = tokenizer.encode_batch(texts)\n",
        "    all_ids.extend([enc.ids for enc in encs])\n",
        "\n",
        "    return np.array(all_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAQpSf9i3923",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p1=fast_encode([text1], fast_tokenizer, maxlen=512)\n",
        "p1 = model.predict(p1)\n",
        "if (replace(p1) == 0):\n",
        "  print (\"Okay contents\")\n",
        "else:\n",
        "  print (\"Contents not permitted\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0ST9ke6kxOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text2=test.comment_text[0]\n",
        "text2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BrJ6uxxCzvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p2=fast_encode([text2], fast_tokenizer, maxlen=512)\n",
        "p2=model.predict(p2)\n",
        "if (replace(p2) == 0):\n",
        "  print (\"Okay contents\")\n",
        "else:\n",
        "  print (\"Contents not permitted\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC_Ro_H_lj8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text3 =[\"Every once in a while, I get the urge. You know what I'm talking about, don't you? The urge for destruction. The urge to hurt, maim, kill. It's quite a thing to experience that urge, to let it wash over you, to give in to it. It's addictive. It's all-consuming. You lose yourself to it. It's quite, quite wonderful. I can feel it, even as I speak, tapping around the edges of my mind, trying to prise me open, slip its fingers in. And it would be so easy to let it happen. But we're all like that, aren't we? We're all barbarians at our core. We're all savage, murderous beasts. I know I am. I'm sure you are. The only difference between us, Mr. Prave, is how loudly we roar. I know I roar very loudly indeed. How about you. Do you think you can match me\"]\n",
        "text3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1sAPCz_pwXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p3=fast_encode(text3, fast_tokenizer, maxlen=512)\n",
        "p3=model.predict(p3)\n",
        "if (replace(p2) == 0):\n",
        "  print (\"Okay contents\")\n",
        "else:\n",
        "  print (\"Contents not permitted\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}